---
title:  "New York City Subway - Yankee Stadium Volume"
date: 2020-01-06
tags: [Time Series, Machine Learning, Data Science]
excerpt:  "New York City Subway - Yankee Stadium Volume"  
---

The general "go-to" for a time series problem is to go right to Autoregressive Integrated Moving Average (ARIMA) models.  Following ARIMA models, analysts versed in deep learning often move on to Recurrent Neural Networks (RNNs).  

Is this a "good" approach?  

Arguably no.

ARIMA and RNN models assume that previous values of the target variable have an influence on present and future values.  Many analysts assume this is the case. In fact, it may *not* be the case.

Target values in time series may have a strong relationship directly to features at prediction time.  Accordingly, if one has (or can meaningfully predict) feature values at future times, target values can be predicted through straightforward regression methods.

This is the case with New York City Subway exit values at the Yankee Stadium Station.  

The first analyst impulse is to treat this time series data using time series modelling.  However, as shown, volume has less to do with prior volumes and more to do with exogenous features.

# ARIMA, RNN, and 'Straightforward' Regression Models

ARIMA and RNN models are used when an analyst hypothesizes that future target values are influences, at least in part, by past values.  

The theory behind these models is that the current and future target values are influenced by past target values or prediction errors.  

ARIMA models assume an autocorrelation between the current target an prior target values (the 'AR' portion of the model) and the prediction errors on prior target values (the 'MA' of the model).  
<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/ARIMA-formula.jpg" alt="">

Similarly, RNNs are predicated on the present value relying on current input values in addition to prior outputs.  In RNN neurons, previous outputs are combined with current period inputs before passing through an activation function to generate an output.

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/RNN-formula.jpg" alt="">

Straightforward regression *does not* assume that past targets impact the present value.   The assumption is only inputs from the *present time* predict the current outcome.  What impact the present inputs have is determined by training the model on inputs during prior periods.  A CART Regression Tree, the basis for a Random Forest Regressor, is shown below.

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/cart.png" alt="">

Note that model inputs *do not reflect* features from a prior time.  The model is trained using feature values that correspond to a particular output.  Predictions, when generated, involve running a set of features down the tree to determine the predicted output value.  

# The Data #

## Turnstile Data ##

New York City Subway turnstile data is available
[here](http://web.mta.info/developers/turnstile.html).

The website contains weekly data files with entry and exit volume by station/turnstile for every unit in the NYC Subway system in four hour increments.  The format is as follows:  

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/NYC-Subway-Time-Series-Data.jpg" alt="">

* CA / SCP / UNIT - Coded identifying features that identfy turnstile units at particular stations.  (New York City MTA provides keys in a file at the above link.)

* DATE - The date of station activity

* TIME - the four hour period during which activity took place (The figure above shows these periods at 22 minutes after hours 4, 8, 12, 16, and 00.  In fact, most stations/turnstiles are timestamped on the four hour increments.  This difference is dealt with in data preprocessing.)

* DESC - The type of data entry.  "REGULAR" for regular periodic reporting.  Another label for corrections.

* ENTRIES - The number of people entering through the turnstile.

* EXITS - The number of people exiting through the turnstile.

## Feature Engineering ##

### Datetime Index ###

DATE and TIME are two separate features.  This is great for regression modes but not time series models.  To properly utilize ARIMA and RNN models we need a datetime index in time stamp form.  We generate this as follows:

```python

# Make a datetime object with date and hours from date and hours columns, set it to the dataframe
#index.  Drop the original date and hour columns.

season_2013_df['DATE'] = season_2013_df['DATE'].apply(lambda x: x.strftime('%Y-%m-%d'))
season_2013_df['DATETIME'] = season_2013_df['DATE'] + " " + season_2013_df['TIME']
season_2013_df['DATETIME'] = pd.to_datetime(season_2013_df['DATETIME'], errors='coerce')
season_2013_df = season_2013_df.set_index('DATETIME')
season_2013_df.drop(['DATE','TIME'],axis=1,inplace=True)
```

Which gives us:

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/datetime_idx.jpg" alt="">

### Teams and Homegames ###

The team the Yankees are playing may have a pronounced impact on game attendance and therefore station volume.

Regular season game information can be found [here.](https://www.baseball-reference.com/teams/NYY/2013-schedule-scores.shtml)

Using the information we can identify the date and time each team played at Yankee Stadium during the regular season.

Homegame and Team are both categorical variables.  

HOMEGAME is created by creating a binary indicator displaying whether or not a home game begins during particular 4 hour bans.  Because game time information was not available and because most games begin in the afternoon or early evening, a value of 1 is assigned to the 16 and 20 hour time bands on dates which the Yankees have home games.  Other time bands are assigned a value of 0.

TEAM is created by label encoding team opposing team abbreviations founds in the Yankees 2013 schedule.  Team labels are assigned where HOMEGAME values equal 1.  

```python
#Use Pandas label encoder function to change team strings to integer values.  This will allow
#for processing in sklearn.ensemble Random Forest Regressor

teams = season_2013_df['TEAM'].unique()
le = preprocessing.LabelEncoder()
le.fit(teams)
print (pd.DataFrame({'Teams':le.classes_}))
season_2013_df['TEAM'] = le.transform(season_2013_df['TEAM'])

```
Which gives us:

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/team_idx.jpg" alt="">

Note that a "No Game" label is included which is assigned to observations where there is no home game (HOMEGAME = 0).

### DAYOFWEEK and HOUR ###

The day of week and hour of day of turnstile timestamp might also be helpful for some models.  These are easily created using  `datetime`.

```python
season_2013_df['DAYOFWEEK'] = season_2013_df['DATETIME'].dt.dayofweek
season_2013_df['HOUR'] = season_2013_df['DATETIME'].dt.hour
```

`datetime.dayofweek` returns an encoded value between 0 and 6 depending on the day of the week.  Monday begins the week and is encoded as 0.  Subsequent week days are encoded sequentially.

`datetime.hour` returns an encoded value corresponding to the hour of the day based on the 24 hour time clock.  

### ENTRIES ###

ENTRIES has limited use.  Historical entry values are known and could - hypothetically and incorrectly - be used to train a model and make predictions on a validation data set.  However, actual ENTRIES values for true predictions into the future cannot be used because they don't yet exist.  The only way such values could be used would be if they could be accurately predicted prior to running a model.  While possible, that process is beyond the scope of this article.

Accordingly, ENTRIES are dropped from the data set.

## Data with Engineered Features ##

The data with engineered features and with unnecessary columns removed is as follows:

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/engineered_features.jpg" alt="">

## One Hot Encoding ##

TEAMS, DAYOFWEEK, and HOUR are all sequentially encoded.  Models may interpret this encoding as hierarchical which would be incorrect.  Accordingly, these features must be one hot encoded.  The following code is utilized.

```python
#One Hot Encode TEAM, DAYOFWEEK, HOUR

TEAM_ohe = OneHotEncoder()
DAYOFWEEK_ohe = OneHotEncoder()
HOUR_ohe = OneHotEncoder()
TEAM_ohe_array = TEAM_ohe.fit_transform(season_2013_df.TEAM.values.reshape(-1,1)).toarray()
DAYOFWEEK_ohe_array = DAYOFWEEK_ohe.fit_transform(season_2013_df.DAYOFWEEK.values.reshape(-1,1)).toarray()
HOUR_ohe_array = HOUR_ohe.fit_transform(season_2013_df.HOUR.values.reshape(-1,1)).toarray()
```

After re-encorporating the one hot encoded data into the original dataframe and removing unnecessary columns the data set 33 features:

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/final_dataframe_columns.jpg" alt="">
