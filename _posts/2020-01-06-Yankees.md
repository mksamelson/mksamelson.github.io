---
title:  "Yankee Stadium Subway Volume"
date: 2020-01-06
tags: [Time Series, Machine Learning, Data Science]
excerpt:  "Yankee Stadium Subway Volume"  
---

The general "go-to" for a time series problem is to go right to Autoregressive Integrated Moving Average (ARIMA) models.  Following ARIMA models, analysts versed in deep learning often move on to Recurrent Neural Networks (RNNs).  

Is this a "good" approach?  

Arguably no.

ARIMA and RNN models assume that previous values of the target variable have an influence on present and future values.  Many analysts assume this is the case. In fact, it may *not* be the case.

Target values in time series may have a strong relationship directly to features at prediction time.  Accordingly, if one has (or can meaningfully predict) feature values at future times, target values can be predicted through straightforward regression methods.

This is the case with New York City Subway exit values at the Yankee Stadium Station.  

The first analyst impulse is to treat this time series data using time series modelling.  However, as shown, volume has less to do with prior volumes and more to do with exogenous features.

# ARIMA, RNN, and 'Straightforward' Regression Models

ARIMA and RNN models are used when an analyst hypothesizes that future target values are influences, at least in part, by past values.  

The theory behind these models is that the current and future target values are influenced by past target values or prediction errors.  

ARIMA models assume an autocorrelation between the current target an prior target values (the 'AR' portion of the model) and the prediction errors on prior target values (the 'MA' of the model).  
<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/ARIMA-formula.jpg" alt="">

Similarly, RNNs are predicated on the present value relying on current input values in addition to prior outputs.  In RNN neurons, previous outputs are combined with current period inputs before passing through an activation function to generate an output.

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/RNN-formula.jpg" alt="">

Straightforward regression *does not* assume that past targets impact the present value.   The assumption is only inputs from the *present time* predict the current outcome.  What impact the present inputs have is determined by training the model on inputs during prior periods.  A CART Regression Tree, the basis for a Random Forest Regressor, is shown below.

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/cart.png" alt="">

Note that model inputs *do not reflect* features from a prior time.  The model is trained using feature values that correspond to a particular output.  Predictions, when generated, involve running a set of features down the tree to determine the predicted output value.  

# The Data #

## Turnstile Data ##

New York City Subway turnstile data is available
[here](http://web.mta.info/developers/turnstile.html).

The website contains weekly data files with entry and exit volume by station/turnstile for every unit in the NYC Subway system in four hour increments.  The format is as follows:  

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/NYC-Subway-Time-Series-Data.jpg" alt="">

* CA / SCP / UNIT - Coded identifying features that identfy turnstile units at particular stations.  (New York City MTA provides keys in a file at the above link.)

* DATE - The date of station activity

* TIME - the four hour period during which activity took place (The figure above shows these periods at 22 minutes after hours 4, 8, 12, 16, and 00.  In fact, most stations/turnstiles are timestamped on the four hour increments.  This difference is dealt with in data preprocessing.)

* DESC - The type of data entry.  "REGULAR" for regular periodic reporting.  Another label for corrections.

* ENTRIES - The number of people entering through the turnstile.

* EXITS - The number of people exiting through the turnstile.

## Feature Engineering ##

### Datetime Index ###

DATE and TIME are two separate features.  This is great for regression modes but not time series models.  To properly utilize ARIMA and RNN models we need a datetime index in time stamp form.  We generate this as follows:

```python

# Make a datetime object with date and hours from date and hours columns, set it to the dataframe
#index.  Drop the original date and hour columns.

season_2013_df['DATE'] = season_2013_df['DATE'].apply(lambda x: x.strftime('%Y-%m-%d'))
season_2013_df['DATETIME'] = season_2013_df['DATE'] + " " + season_2013_df['TIME']
season_2013_df['DATETIME'] = pd.to_datetime(season_2013_df['DATETIME'], errors='coerce')
season_2013_df = season_2013_df.set_index('DATETIME')
season_2013_df.drop(['DATE','TIME'],axis=1,inplace=True)
```

Which gives us:

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/datetime_idx.jpg" alt="">

### Teams and Homegames ###

The team the Yankees are playing may have a pronounced impact on game attendance and therefore station volume.

Regular season game information can be found [here.](https://www.baseball-reference.com/teams/NYY/2013-schedule-scores.shtml)

Using the information we can identify the date and time each team played at Yankee Stadium during the regular season.

Homegame and Team are both categorical variables.  

HOMEGAME is created by creating a binary indicator displaying whether or not a home game begins during particular 4 hour bans.  Because game time information was not available and because most games begin in the afternoon or early evening, a value of 1 is assigned to the 16 and 20 hour time bands on dates which the Yankees have home games.  Other time bands are assigned a value of 0.

TEAM is created by label encoding team opposing team abbreviations founds in the Yankees 2013 schedule.  Team labels are assigned where HOMEGAME values equal 1.  

```python
#Use Pandas label encoder function to change team strings to integer values.  This will allow
#for processing in sklearn.ensemble Random Forest Regressor

teams = season_2013_df['TEAM'].unique()
le = preprocessing.LabelEncoder()
le.fit(teams)
print (pd.DataFrame({'Teams':le.classes_}))
season_2013_df['TEAM'] = le.transform(season_2013_df['TEAM'])

```
Which gives us:

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/team_idx.jpg" alt="">

Note that a "No Game" label is included which is assigned to observations where there is no home game (HOMEGAME = 0).

### DAYOFWEEK and HOUR ###

The day of week and hour of day of turnstile timestamp might also be helpful for some models.  These are easily created using  `datetime`.

```python
season_2013_df['DAYOFWEEK'] = season_2013_df['DATETIME'].dt.dayofweek
season_2013_df['HOUR'] = season_2013_df['DATETIME'].dt.hour
```

`datetime.dayofweek` returns an encoded value between 0 and 6 depending on the day of the week.  Monday begins the week and is encoded as 0.  Subsequent week days are encoded sequentially.

`datetime.hour` returns an encoded value corresponding to the hour of the day based on the 24 hour time clock.  

### ENTRIES ###

ENTRIES has limited use.  Historical entry values are known and could - hypothetically and incorrectly - be used to train a model and make predictions on a validation data set.  However, actual ENTRIES values for true predictions into the future cannot be used because they don't yet exist.  The only way such values could be used would be if they could be accurately predicted prior to running a model.  While possible, that process is beyond the scope of this article.

Accordingly, ENTRIES are dropped from the data set.

## Data with Engineered Features ##

The data with engineered features and with unnecessary columns removed is as follows:

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/engineered_features.jpg" alt="">

## One Hot Encoding ##

TEAMS, DAYOFWEEK, and HOUR are all sequentially encoded.  Models may interpret this encoding as hierarchical which would be incorrect.  Accordingly, these features must be one hot encoded.  The following code is utilized.

```python
#One Hot Encode TEAM, DAYOFWEEK, HOUR

TEAM_ohe = OneHotEncoder()
DAYOFWEEK_ohe = OneHotEncoder()
HOUR_ohe = OneHotEncoder()
TEAM_ohe_array = TEAM_ohe.fit_transform(season_2013_df.TEAM.values.reshape(-1,1)).toarray()
DAYOFWEEK_ohe_array = DAYOFWEEK_ohe.fit_transform(season_2013_df.DAYOFWEEK.values.reshape(-1,1)).toarray()
HOUR_ohe_array = HOUR_ohe.fit_transform(season_2013_df.HOUR.values.reshape(-1,1)).toarray()
```

After re-encorporating the one hot encoded data into the original dataframe and removing unnecessary columns the data set 33 features:

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/final_dataframe_columns.jpg" alt="">

## Training and Validation Sets ##

The entire data set represents the normal Yankees baseball season.  The dates span from April 1, 2013 through September 30, 2013.

The models will be trained on the first five months of the data (April through August) with the six month reserved for validation (September).

Accordingly, EXITS becomes the dependent variable and the 33 features become the independent variables

There are 1097 observations representing the six months of daily data timestamped in 6 four-hour windows per day.  September 1, 2013 at 00:00:00 is the 917th observation.  Accordingly, training and validation sets are created as follows.

```python
y = season_2013_enc_df['EXITS'].values.reshape(-1,1)
X = season_2013_enc_df.iloc[:,1:len(season_2013_enc_df)].values

X_train = X[:917]
X_test = X[917:]
y_train = y[:917]
y_test = y[917:]

```

# Models #

Four models are employed to model this data:

* Seasonal Autoregressive Integrated Moving Average with Exogenous Regressors (SARAMAX)

* Recurrent Neural Networks (RNN)

* Random Forest Regressor

* Artificial Neural Network (ANN)

The SARAMAX and RNN models are used with the hypothesis that the predicted values of the target variable (EXITS) are strongly influenced by earlier values of the target variable.  

The Random Forest Regressor and ANN models are used with the hypothesis that predicted values of the target variable are *not* strongly influenced by earlier values of the target variable. Instead, the existing and engineered feature values *during the window for which the model is predicting* are influential in predicting the target variable value.

## Modeling and results ##

### Seasonal Autoregressive Integrated Moving Average with Exogenous regressors (SARAMAX) ###

SARAMAX is an ARIMA model that utilizes additional features to enhance performance.  ARIMA modeling is done on the time series (in this case the EXITS values).

As an initial step EDA step the EXITS time series is decomposed to examine its level, seasonal, trend and noise components.   

This analysis is shown below on the entire timeseries.  Due to granularity values for only the first 200 observations are shown.

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/seasonal_decomp_200.jpg" alt="">

Dispensing with a detailed discussion of ARIMA models and describing the intuition behind determining model parameters (which is not at all intuitive) and focusing on process, the following is done:

* Determine the period of seasonal differences
* Use `pmdarima.auto_arima` to find preferred AR and MA parameters for an efficient and accurate model.

Deterimining the period of seasonal differences is done manually by further examining the seasonality pane of the time series decomposition.  Magifying the above figure further and closely examining it, the seasonality *appears* to be daily (thus 6 since there are six four-hour periods in a day).  However, this is not the case.  Careful review shows that the period of seasonal differences is, in fact, 24.  

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/magnified_seasonality.jpg" alt="">

`auto_arima` is a function in the `pmdarima` package.  In short, it ports supurb R language ARIMA model functionality into Python.  the `auto_arima` function Returns best ARIMA model parameters according to either AIC, AICc or BIC value.  Here I utilized AIC.  The AIC metric takes both accuracy and model complexity into account.  Running the function on the EXITS timeseries yielded the following (partial) best model summary.

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/SARAMAX_summary.jpg" alt="">

The parameters of the optimal SARIMA model are shown in the highlighted yellow section of the above figure.

The model is trained as follows and predictions generated.

```Python
model = SARIMAX(train_df['EXITS'],exog=train_df[['HOMEGAME','TEAM','DAYOFWEEK']],order=(3,0,2),seasonal_order=(2,0,1,24),
                trend = 'ct', enforce_invertibility=False)
results = model.fit()

# Obtain predicted values
start=len(train_df.index)
end=len(train_df.index)+len(test_df.index)-1
exog_forecast = test_df[['HOMEGAME','TEAM','DAYOFWEEK']]
predictions = results.predict(start=start, end=end, exog=exog_forecast,dynamic=False).rename('SARIMAX(3,0,2)(2,0,1,24) Predictions')
```

The results of the validation set are as follows.

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/SARAMAX_validation_results.jpg" alt="">

RMSE:  1933.30

Cleary the SARIMAX model does well capturing seasonality of the timeseries.  However, it is clearly limited in fully capturing spikes in EXIT volume.  Thus autocorrelation has some informative value in prediction and exogenous variables help as results using a SARIMAX model are better than simply using a SARIMA model (not shown).  

## Artificial Neural Network (ANN)

An artificial Neural Network utilizes the power of hidden neuron layers to capture non-linear relationships between inputs and outputs.  unlike the Recursive Neural Network (discussed below), it does not consider model outputs from prior time periods and thus can be considered a fundamental classification/regression model.

Accordingly, EXITS is not used as an input in the ANN.  TIME, DAYOFWEEK, HOMEGAME, and TEAM variables are used to predict EXIT value.

```Python
model = Sequential()

model.add(Dense(256, input_dim = X_train.shape[1], activation='relu'))
# 16 Neurons (Play around with this number!)
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='linear'))

# Compile the layers
model.compile(loss='mse', metrics=[RootMeanSquaredError(name='rmse')], optimizer='adam')

```

The structure of the ANN uses 4 deep layers of neurons, each consecutively smaller (the first layer with 128 neurons, the second 64, etc.).  The final layer simply provides a single numerical output - the EXIT value for a particular date/time.  

This is a straightforward neural network model.  It is a bit unusual in that it has more layers than most models (arguably 2 hidden layers is reputed to be sufficient for most business problems) and there have been no significant tuning methods applied (eg., neuron "dropping").

Validation set results for the model are as follows.

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/ANN_validation_results.jpg" alt="">

RMSE:  1688.69

The ANN does a much better job capturing the observed EXITS spikes.  While not perfect, the ANN does well at capturing the amplitude of values across time periods even though it does not incorporate relationships between the predicted and prior EXITS values.  The SARIMAX model above clearly indicates that there is an autocorrelative effect on output which is *not* included in this ANN.  There are ways to incorporate lagged EXIT values into this model but for simplicity it is not done.

## Recursive Neural Network (RNN)

Like the SARIMAX model above, the RNN is a deep learning model that is particularly useful for time series data.  

Like the ANN above, the RNN uses a series of hidden layers which ultimately produce predicted output values.

Unlike the ANN, the RNN is similar to the SARIMAX model in that it tries to find the relationship between prior time period EXITS values to predict future EXITS values.  This model also does not incorporate exogenous variables.

Accordingly, the RNN focuses entirely on a set period of prior EXITS observations to predict the next EXITS value.  

There are two RNN modeling attempts here.  One uses a number of preceding EXITS values to predict *only* the next EXITS value.  The other uses a series of preceding EXITS values to predict *numerous* future EXITS values.

The key difference in these two models is the data preparation.  The "next period" RNN model uses the *actual* T preceding time series values.  The multi-period model incorporates *predicted* values in the T preceding time series values to predict future values.  This is because a prediction six periods in the future can't use actual values to make the prediction because the five future values don't yet exist. 

```python


```
