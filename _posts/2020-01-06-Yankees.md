---
title:  "New York City Subway - Yankee Stadium Volume"
date: 2020-01-06
tags: [Time Series, Machine Learning, Data Science]
excerpt:  "A lot has been written about making predictions using time series data.  The general "go-to" when one sees a time series is to go right to ARIMA (Autoregressive Integrated Moving Average) models.  If ARIMA results are sub-par, standard practice is to move on to RNNs (Recurrent Neural Networks).  Yet is this a "good" approach?"  
---

The general "go-to" for a time series problem is to go right to Autoregressive Integrated Moving Average (ARIMA) models.  Following ARIMA models, analysts versed in deep learning often move on to Recurrent Neural Networks (RNNs).  

Is this a "good" approach?  

Arguably no.

ARIMA and RNN models assume that previous values of the target variable have an influence on present and future values.  Many analysts assume this is the case. In fact, it may *not* be the case.

Target values in time series may have a strong relationship directly to features at prediction time.  Accordingly, if one has (or can meaningfully predict) feature values at future times, target values can be predicted through straightforward regression methods.

This is the case with New York City Subway exit values at the Yankee Stadium Station.  

The first analyst impulse is to treat this time series data using time series modelling.  However, as shown, volume has less to do with prior volumes and more to do with exogenous features.

# ARIMA, RNN, and 'Straightforward' Regression Models

ARIMA and RNN models are used when an analyst hypothesizes that future target values are influences, at least in part, by past values.  

The theory behind these models is that the current and future target values are influenced by past target values or prediction errors.  

ARIMA models assume an autocorrelation between the current target an prior target values (the 'AR' portion of the model) and the prediction errors on prior target values (the 'MA' of the model).  
<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/ARIMA-formula.jpg" alt="">

Similarly, RNNs are predicated on the present value relying on current input values in addition to prior outputs.  In RNN neurons, previous outputs are combined with current period inputs before passing through an activation function to generate an output.

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/RNN-formula.jpg" alt="">

Straightforward regression *does not* assume that past targets impact the present value.   The assumption is only inputs from the *present time* predict the current outcome.  What impact the present inputs have is determined by training the model on inputs during prior periods.  A CART Regression Tree, the basis for a Random Forest Regressor, is shown below.

<img src="{{site.url}}{{ site.baseurl }}/images/timeseries/cart.png" alt="">

Note that model inputs *do not reflect* features from a prior time.  The model is trained using feature values that correspond to a particular output.  Predictions, when generated, involve running a set of features down the tree to determine the predicted output value.  
