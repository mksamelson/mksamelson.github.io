---
title:  "Predicting Customer Churn"
date: 2019-11-20
tags: [Classification, Machine Learning, Data Science]
excerpt:  #"Predicting Customer Churn"
---

Customer retention is critical for all businesses.  Customers are the primary source of company revenue and
have a direct impact on the bottom line. Understanding engagement - and better yet being able to predict
engagement - can be particularly helpful to intercede in issues **before** they become problems.

In this post we examine three months of real pre-paid phone card data and develop a model to predict
customers who will drop the service in the fourth month.

All work is done in R.

## The Data

The data set consists of three months of pre-paid phone card data from June through August 2013.  There
are approximately 65,000 observations reflecting individual customers and 65 features.  The objective
is to use the June, July, and August customer data to predict customers who will drop the service in
September.

Historical, churn, and descriptive information is contained in a single excel file. One page contains various categorical and continuous factors on customer spending, phone use, SMS use, data use, and some other information. A second page contains a binary churn indicator showing which customers left in September 2013. A third page has definitions of variables.

## Exploratory Data Analysis (EDA)

## Model

After munging and doing preliminary analysis of the historical data set, I chose to utilize a boosted tree model to predict churn. I utilized the xgbtree method in the Caret package which makes use of XgBoost. Boosted trees are exceptionally powerful and the XgBoost package is known to be particularly powerful. I used the Caret package to access XgBoost because I find data pre-processing, using cross validation in model training, and performing a grid search to obtain optimal model parameters to be easier to use in Caret than in XgBoost proper.

The best model had an AUC of approximately .92 and an error of approximately .11 using the accuracy metric.

To address the business problem (i.e., identify and contact customers who will leave in a subsequent month based) I found it best to use an ROC cutoff that balanced specificity and sensitivity and resulted in slightly lower accuracy but yielded a preferable mix of true positives and false positives beneficial to addressing the business problem.

Training the model is time and resource intensive. The model was trained in an AWS instance with 8G RAM and 30G memory. Run time in that environment was approximately one hour.

Since training the model may not be practical in terms of time and resources I have provided the analytics output as comments in the code file. Plots are in their own separate files in the repository. Unfortunately the trained model file was too large to upload to the Github repository.
